Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost): org.bson.json.JsonParseException: JSON reader was expecting a value but found 'Imen'.
	at org.bson.json.JsonReader.readBsonType(JsonReader.java:251)
	at org.bson.codecs.DocumentCodec.decode(DocumentCodec.java:149)
	at org.bson.codecs.DocumentCodec.decode(DocumentCodec.java:45)
	at org.bson.Document.parse(Document.java:105)
	at org.bson.Document.parse(Document.java:90)
	at com.vermeg.migrate.SparkProducerCSV$1.call(SparkProducerCSV.java:34)
	at com.vermeg.migrate.SparkProducerCSV$1.call(SparkProducerCSV.java:31)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:914)
	at scala.collection.Iterator$GroupedIterator.go(Iterator.scala:929)
	at scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:968)
	at scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:972)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at com.mongodb.spark.MongoSpark$$anonfun$save$1$$anonfun$apply$1.apply(MongoSpark.scala:131)
	at com.mongodb.spark.MongoSpark$$anonfun$save$1$$anonfun$apply$1.apply(MongoSpark.scala:130)
	at com.mongodb.spark.MongoConnector$$anonfun$withCollectionDo$1.apply(MongoConnector.scala:186)
	at com.mongodb.spark.MongoConnector$$anonfun$withCollectionDo$1.apply(MongoConnector.scala:184)
	at com.mongodb.spark.MongoConnector$$anonfun$withDatabaseDo$1.apply(MongoConnector.scala:171)
	at com.mongodb.spark.MongoConnector$$anonfun$withDatabaseDo$1.apply(MongoConnector.scala:171)
	at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
	at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
	at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
	at com.mongodb.spark.MongoSpark$$anonfun$save$1.apply(MongoSpark.scala:130)
	at com.mongodb.spark.MongoSpark$$anonfun$save$1.apply(MongoSpark.scala:129)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$33.apply(RDD.scala:920)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$33.apply(RDD.scala:920)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace: