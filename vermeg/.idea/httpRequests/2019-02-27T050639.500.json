{
  "timestamp": "2019-02-27T16:06:39.000+0000",
  "status": 500,
  "error": "Internal Server Error",
  "message": "Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost): org.bson.json.JsonParseException: JSON reader was expecting a value but found 'imen'.\n\tat org.bson.json.JsonReader.readBsonType(JsonReader.java:251)\n\tat org.bson.codecs.DocumentCodec.decode(DocumentCodec.java:149)\n\tat org.bson.codecs.DocumentCodec.decode(DocumentCodec.java:45)\n\tat org.bson.Document.parse(Document.java:105)\n\tat org.bson.Document.parse(Document.java:90)\n\tat com.vermeg.migrate.SparkProducer$1.call(SparkProducer.java:81)\n\tat com.vermeg.migrate.SparkProducer$1.call(SparkProducer.java:76)\n\tat org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:914)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:929)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:968)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:972)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat com.mongodb.spark.MongoSpark$$anonfun$save$1$$anonfun$apply$1.apply(MongoSpark.scala:131)\n\tat com.mongodb.spark.MongoSpark$$anonfun$save$1$$anonfun$apply$1.apply(MongoSpark.scala:130)\n\tat com.mongodb.spark.MongoConnector$$anonfun$withCollectionDo$1.apply(MongoConnector.scala:186)\n\tat com.mongodb.spark.MongoConnector$$anonfun$withCollectionDo$1.apply(MongoConnector.scala:184)\n\tat com.mongodb.spark.MongoConnector$$anonfun$withDatabaseDo$1.apply(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector$$anonfun$withDatabaseDo$1.apply(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n\tat com.mongodb.spark.MongoSpark$$anonfun$save$1.apply(MongoSpark.scala:130)\n\tat com.mongodb.spark.MongoSpark$$anonfun$save$1.apply(MongoSpark.scala:129)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$33.apply(RDD.scala:920)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$33.apply(RDD.scala:920)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:",
  "path": "/sparkpi"
}